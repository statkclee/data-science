---
layout: page
title: 데이터 사이언스
subtitle: "데이터 사이언스 파이프라인 - Base vs. Tidyverse"
date: "`r Sys.Date()`"
author:
    name: xwMOOC
    url: https://www.facebook.com/groups/tidyverse/
    affiliation: Tidyverse Korea
output:
  html_document: 
    toc: yes
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
editor_options: 
  chunk_output_type: console
---


``` {r, include=FALSE}
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                    comment="", digits = 3, tidy = FALSE, prompt = FALSE, fig.align = 'center')
library(reticulate)
use_condaenv("anaconda3")
# reticulate::repl_python()
```

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 10px;}
</style>
<div class = "blue">

**데이터 분석 정의**

데이터 분석은 데이터가 이해되고, 지식이 되고, 통찰을 얻게 되는 과정이다.  

* "Data analysis is the process by which data becomes understanding, knowledge and insight"*    

<br>
--- Hadley Wickham
</div>


# 데이터 사이언스 파이프라인 [^before-and-after] [^pipeline-webinar] {#data-science-pipeline}

[^pipeline-webinar]: [Pipelines for data analysis in R](https://www.rstudio.com/resources/webinars/pipelines-for-data-analysis-in-r/)

[^before-and-after]: [Jared P. Lander (2019) "R: Then and Now", New York R Conference](https://www.youtube.com/watch?v=gjpNEVcG1nU)

데이터 분석은 다른 소프트웨어 개발과 동일하게 **컴퓨터 시간(Computer time)**과 
**개발자 시간(Human time)**을 최적화하는 과정으로 볼 수 있다.
최근에 가장 희귀하고 귀중한 자원은 컴퓨터가 아니라 개발자 시간 즉, 사람이다.

따라서, 사람 시간을 최적화하는 것이 데이터 분석에서 가장 중요한 요소가 되면 그 해답이
**%>%** 즉, `magrittr` 팩키지에도 존재한다.
프랑스 액센트가 들어간 [magrittr](https://cran.r-project.org/web/packages/magrittr/) 팩키지는 
개발시간을 단축하고 코드 가독성을 높이고 유지보수성을 높이는 목적을 갖고 있다.
유닉스 쉘, `F#`, haskell, clojure, method chaining에서 영감을 받았다.

<img src="fig/ds-pipeline.png" alt="데이터 과학 파이프라인" width="77%" />

파일(`.csv`같은 일반 텍스트 파일, 엑셀같은 이진 파일) 형태, 데이터베이스(SQL, NoSQL) 형태,
웹데이터(JSON, XML) 형태로 존재하는 데이터를 가져와서 이를 분석에 적합한 형태, 즉 **깔끔한 데이터(tidy data)**로 
변환을 해야만 한다. 깔끔한 데이터는 기계도 잘 이해하고, 분석가도 잘 이해하는 형태가 되어야 한다.

|   기계 저장 방식   |     사람 이해 방식    |
|:------------------:|:---------------------:|
|   파일/테이블      |    데이터셋(Data set) |
|      행(row)       |   관측점(Observation) |
|    열(column)      |    변수(Variable)     |


# 데이터 사이언스 통합개발환경 {#ds-ide}

1990년대 초반 탄생한 R 언어는 초기 빈약하고 불편한 개발환경에서 진화에 진화를 거듭하여 RStudio 통합개발환경을 통해 4개의 작은창(pane)을 통해 데이터 사이언스 세계로 들어가는 포탈을 열었다는 평가를 받고 있다.

<img src="fig/ide-evolution.png" alt="통합개발환경 IDE" width="100%" />

# 해들리 위컴과 브라이언 리플리 {#ds-who}

[Brian D. Ripley](https://en.wikipedia.org/wiki/Brian_D._Ripley)는 "S Programming", "Modern Applied Statistics with S" 두권의 책으로 잘 알려져 있으면 베이스(Base) R로 알려진 핵심 코드의 상당부분을 저작한 것으로 유명하다. [Hadley Wickham](https://en.wikipedia.org/wiki/Hadley_Wickham)은 #JSM2019 에서 COPSS award를 수상하며 그동안 `tidyverse` 구축에 공헌한 것을 인정받았으며 최근 R 생태계 구축 및 R 코드 상당부분을 저작하여 기여했다.

<img src="fig/brian-hadley.png" alt="선수교체" width="100%" />

# 데이터 사이언스 팩키지- `tidyverse` {#tidyverse-packages}

데이터 과학을 위한 파이프라인은 유닉스 파이프와 마찬가지로 
각 모듈별로 통일된 인터페이스를 유지해야만, 
앞선 선행작업을 후행작업과 연결하거나 조합하여 사용한다. 처음으로 데이터를 가져와서 최종적으로 
의사소통하는 일련의 과정에 데이터 과학 각 모듈별로 공통된 인터페이스를 갖고 이를 조합하여 연결해내는 
과정을 `tidyverse` 팩키지에 수많은 데이터과학자의 노력이 녹아져 있다.

* **데이터 가져오기**
    - readr : 데이터 가져오기
    - DBI : 데이터베이스
    - haven: SPSS, SAS, Stata
    - httr: 웹 APIs
    - jsonlite: JSON
    - readxl: 엑셀
    - rvest: 웹스크래핑
    - xml2: XML
    - readr : 데이터 가져오기
* **데이터 정제**
    - tidyr : 데이터 깔끔화
    - dplyr : 데이터 조작
    - ggplot2 : 데이터 시각화
    - ggvis: 인터랙티브 시각화
    - purrr : 함수형 프로그래밍
* **데이터 자료형**
    - tibble : 최신 데이터프레임
    - hms : 시간 자료형
    - stringr : 문자열 자료형
    - lubridate : 날짜/시간 자료형
    - forcats : 요인 자료형
* **모형**
    - modelr : 파이프라인 내부 모형개발
    - broom : 모형산출물을 깔끔한 데이터로 변환
* **의사소통**
    - rmakrdown : 마크다운 문서화
    - bookdown : 다양한 출력물 산출(pdf, html, ePub등)
    - flexdashboard : 정적 인터랙티브 대쉬보드
    - shiny : 웹응용프로그램, 동적 대쉬보드

<img src="fig/ds-tidyverse.png" alt="깜끔한 세상" width="97%" />

# `Base R`과 `tidyverse` 비교 {#base-r-tidyverse}

## `magrittr` 코드와 일반 R 코드 비교 [^magrittr-vignette] {#function}

전통적인 R코드는 본인이 작성하지 않았다면 해독하기가 만만치 않고, 괄호가 많다.
결국 읽기 어렵고 이해하기 힘든 함수 조합을 가독성 높은 순열(sequence)로 변환하는 역할을 한다.

``` {r message=FALSE, warning=FALSE, comment=FALSE, results="hide", eval=FALSE}
x %>% f(y)
# f(x, y)
x %>% f(z, .)
# f(z, x)
x %>% f(y) %>% g(z)
# g(f(x, y), z)
```

`mtcars` 데이터셋을 파이프를 통해 데이터를 깔끔하게 정리하는 것과 기존 전통적 방식으로 개발된 코드를 비교해 보자.

<div class = "row">
  <div class = "col-md-6">
**`tidyverse`**

``` {r magrittr-nyc}
library(magrittr)

mtcars %>%
  subset(hp > 100) %>%
  aggregate(. ~ cyl, data = ., FUN = . %>% mean %>% round(2)) %>%
  transform(kpl = mpg %>% multiply_by(0.4251)) %>%
  print
```

  </div>
  <div class = "col-md-6">
**`Base R`**

``` {r base-r-function}
transform(aggregate(. ~ cyl, 
                    data = subset(mtcars, hp > 100), 
                    FUN = function(x) round(mean(x, 2))), 
          kpl = mpg * 0.4251)
```

  </div>
</div>


## 깔끔한 데이터: `tidyr` {#tidyverse-tidyr}

지저분한(messy) 데이터를 데이터 사이언스 작업에 적합한 형태로 `tidyr` 팩키지 핵심 동사 `gather`, `spread`, `separate`, `unite` 등을 활용하여 깔끔한 데이터로 파이프 연산자와 함께 연결시켜 변환시킨다.

``` {r tidyverse-tidyr}
library(tidyverse)

# 데이터 가져오기
tb <- read_csv("https://raw.githubusercontent.com/hadley/tidyr/master/vignettes/tb.csv")

tb %>% 
  DT::datatable()

# 깔끔한 데이터 변환
tidy_tb <- tb %>%
  gather(demographic, n, m04:fu, na.rm = TRUE) %>% 
  separate(demographic, c("sex", "age"), 1) %>%  # 변수를 성별과 연령으로 쪼갠다.
  rename(country = iso2) %>%
  arrange(country, year, sex, age)

tidy_tb %>% 
  sample_n(100) %>% 
  DT::datatable()
```

## 변환: `dplyr` {#tidyverse-dplyr}

- `select`: 데이터테이블에서 변수를 뽑아낸다.
- `filter`: 값으로 관측점을 뽑아낸다.
- `mutate`: 신규 변수를 생성한다. (log 변환)
- `summarise`: 관측점을 하나로 축약한다. (평균)
- `arrange`: 관측점을 오름차순, 내림차순으로 정렬한다.



### 2.3. 시각화 -- `ggvis`

- ggplot2
- shiny
- dplyr 파이프라인
- vega 

### 2.4. 모형 -- `broom`

모형은 알려진 패턴을 제거하는데 탁월하다.

``` {r message=FALSE, warning=FALSE, comment=FALSE, eval=FALSE}
suppressMessages(suppressWarnings(library(tidyr)))
suppressMessages(suppressWarnings(library(ggplot2)))
suppressMessages(suppressWarnings(library(broom)))

tx <- 
  readRDS("tx-housing.rds") %>%
  mutate(date = year + (month - 1) / 12) %>%
  filter(!(city %in% c("Texas Totals", "Palestine")))
tx

ggplot(tx, aes(date, log(sales))) + 
  geom_line(aes(group = city), alpha = 1/2)
ggsave("sales.pdf", width = 8, height = 6)

# Models as a tool --------------------------------------------------------
tx <- tx %>% 
  group_by(city) %>% 
  mutate(
    resid = 
      lm(log(sales) ~ factor(month), na.action = na.exclude) %>%
      resid()
  )

ggplot(tx, aes(date, resid)) +
  geom_line(aes(group = city), alpha = 1/5) + 
  geom_line(stat = "summary", fun.y = "mean", colour = "red")
```

### 5. 빅데이터 

| 구분  |  데이터 크기 |
|--------------|--------------|
| 매우 큰 데이터 | 컴퓨터 한대 메모리에 저장될 수 없는 크기 : **>5 TB** |
| 중간 데이터 | 서버 컴퓨터 메모리에 저장할 수 있는 크기 : **10 GB -- 5 TB** |
| 작은 데이터 | 노트북 컴퓨터 메모리에 저장할 수 있는 크기 : **<10 GB |

전통적으로 R은 작은 데이터를 빠르게 탐색하는데 최적의 환경을 제공한다.
중간크기 데이터를 작업하지 못할 기술적인 이유는 없지만, 거의 작업이 이뤄지지 않고 있다.

빅데이터를 처리하는 전략

1. 요약/표본추출/부분집합 추출 (90%)
2. 작은 데이터 문제로 쪼개서 분할 정복 (9%)
3. 더이상 어찌할 수 없는 큰 문제 데이터 (1%)

### 참고자료

[^magrittr-vignette]: [Library magrittr](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)